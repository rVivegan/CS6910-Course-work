{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "fF_syoJuP_-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "VwpCnaQTG3fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "YbL_2p_dQCLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "train_df = pd.read_csv('train.csv')  # Columns: Sentence, Tags\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "train_sentences = [sent.split() for sent in train_df['Sentence']]\n",
        "train_tags = [tag_seq.split() for tag_seq in train_df['Tags']]\n",
        "test_sentences = [sent.split() for sent in test_df['Sentence']]\n",
        "test_tags = [tag_seq.split() for tag_seq in test_df['Tags']]\n",
        "\n",
        "# Vocabulary and tag mapping\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "idx = 2\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "tag2idx = {\"<PAD>\": 0}\n",
        "idx = 1\n",
        "for tag_seq in train_tags:\n",
        "    for tag in tag_seq:\n",
        "        if tag not in tag2idx:\n",
        "            tag2idx[tag] = idx\n",
        "            idx += 1\n",
        "idx2tag = {v: k for k, v in tag2idx.items()}\n"
      ],
      "metadata": {
        "id": "qF6AVYuiHJVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Embedding setup"
      ],
      "metadata": {
        "id": "1cuNoLAjQLeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare GloVe embedding matrix\n",
        "embedding_matrix = np.random.randn(len(word2idx), 200)\n",
        "glove_path = 'glove.6B.200d.txt'\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.split()\n",
        "        word = parts[0]\n",
        "        vector = np.array(parts[1:], dtype=np.float32)\n",
        "        if word in word2idx:\n",
        "            embedding_matrix[word2idx[word]] = vector\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "2qZIwdIjHp6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n"
      ],
      "metadata": {
        "id": "sdpn4f7yQUXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and DataLoader definitions\n",
        "class PosDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx][:self.max_len]\n",
        "        tag = self.tags[idx][:self.max_len]\n",
        "        x = [self.word2idx.get(word, 1) for word in sent]\n",
        "        y = [self.tag2idx.get(t, 0) for t in tag]\n",
        "        x += [self.word2idx[\"<PAD>\"]] * (self.max_len - len(x))\n",
        "        y += [self.tag2idx[\"<PAD>\"]] * (self.max_len - len(y))\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "train_data = PosDataset(train_sentences, train_tags, word2idx, tag2idx)\n",
        "test_data = PosDataset(test_sentences, test_tags, word2idx, tag2idx)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n"
      ],
      "metadata": {
        "id": "5MpG9Cr1HyQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "vj91aMqlQovq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN POS Tagger Model\n",
        "class RNNTagger(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, tagset_size):\n",
        "        super().__init__()\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "2NiadfG3IDrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "rwLKUml_WBsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "def calculate_accuracy(preds, labels):\n",
        "    mask = labels != tag2idx[\"<PAD>\"]\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "    correct = (preds == labels) & mask\n",
        "    return correct.sum().item() / mask.sum().item()\n",
        "\n",
        "model = RNNTagger(embedding_matrix, 25, len(tag2idx))\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_losses, test_accuracies, train_accuracies = [], [], []\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)\n",
        "        loss = loss_fn(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n",
        "        acc = calculate_accuracy(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "    train_accuracies.append(epoch_acc / len(train_loader))\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            outputs = model(x_batch)\n",
        "            acc = calculate_accuracy(outputs, y_batch)\n",
        "            test_acc += acc\n",
        "            preds = torch.argmax(outputs, dim=-1).view(-1)\n",
        "            labels = y_batch.view(-1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    test_accuracies.append(test_acc / len(test_loader))\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {epoch_loss:.4f} | Train Acc: {train_accuracies[-1]*100:.2f}% | Test Acc: {test_accuracies[-1]*100:.2f}%\")\n",
        "\n",
        "labels_filtered = [p for p, l in zip(all_preds, all_labels) if l != tag2idx[\"<PAD>\"]]\n",
        "true_filtered = [l for l in all_labels if l != tag2idx[\"<PAD>\"]]\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_filtered, labels_filtered, target_names=[idx2tag[i] for i in sorted(idx2tag) if i != tag2idx[\"<PAD>\"]]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_accuracies, label='Train Acc')\n",
        "plt.plot(test_accuracies, label='Test Acc')\n",
        "plt.title(\"Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fgn7zOuHV2ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "rXjowTuEWH1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference: Predict tags for sample sentences\n",
        "def preprocess_sentence(sent, word2idx, max_len=10):\n",
        "    unk_idx = word2idx.get(\"<UNK>\", 1)\n",
        "    pad_idx = word2idx.get(\"<PAD>\", 0)\n",
        "    x = [word2idx.get(word, unk_idx) for word in sent]\n",
        "    x += [pad_idx] * (max_len - len(x))\n",
        "    x = x[:max_len]\n",
        "    return torch.tensor(x).unsqueeze(0)\n",
        "\n",
        "sample_sentences = [\n",
        "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"],\n",
        "    [\"Data\", \"science\", \"is\", \"fun\"]\n",
        "]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sent in sample_sentences:\n",
        "        x_tensor = preprocess_sentence(sent, word2idx, max_len=10).to(next(model.parameters()).device)\n",
        "        output = model(x_tensor)\n",
        "        predictions = torch.argmax(output, dim=-1).squeeze(0)\n",
        "        decoded_tags = [idx2tag[pred.item()] for pred in predictions[:len(sent)]]\n",
        "        print(\"Sentence:\", sent)\n",
        "        print(\"Predicted Tags:\", decoded_tags)\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "qLSxOPN9V7YZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}