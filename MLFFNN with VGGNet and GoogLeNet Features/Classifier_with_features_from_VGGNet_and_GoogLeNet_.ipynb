{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "O9QGGaiABypf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHT30svK4ZOz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "- Load the dataset using your own custom code"
      ],
      "metadata": {
        "id": "QxF38jnECGPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Replace 'file_name.zip' with the name of your zip file\n",
        "zip_file_path = '/content/drive/MyDrive/CS6910/Assignment2/ImageClassification/team_16.zip'\n",
        "\n",
        "extract_to_path = '/content/dataset/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "# Normalize the image data to have mean 0 and standard deviation 1\n",
        "# Resize the image to 224x224\n",
        "# Convert the image to tensor\n",
        "# Create a DataLoader for the dataset\n",
        "# Use a batch size of 32\n",
        "# Shuffle the dataset\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root='./dataset/team_16/train', transform=data_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataset = datasets.ImageFolder(root='./dataset/team_16/test', transform=data_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "nnPK-w1fA_pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing normalized images"
      ],
      "metadata": {
        "id": "BTeXE_dvCI6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_loader.dataset[1000][0].permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c84IDRycBJIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLFFNN Definition and Helper Functions\n"
      ],
      "metadata": {
        "id": "sy2cCQjOCOrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageClassification:\n",
        "    def __init__(self, model, criterion, optimizer, deep_cnn='vgg'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.deep_cnn = deep_cnn\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "\n",
        "\n",
        "    def train(self, train_loader, num_epochs=5):\n",
        "        for epoch in range(num_epochs):\n",
        "            tqdm_train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, (images, labels) in enumerate(tqdm_train_loader):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                predicted = torch.argmax(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                tqdm_train_loader.set_postfix({'Running Loss': running_loss / (i + 1)})\n",
        "                tqdm_train_loader.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            train_accuracy = correct / total\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_accuracy)\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "            # save for each epoch\n",
        "            self.save_model(f'models/Q1/{self.deep_cnn}_epoch_{epoch+1}.pth')\n",
        "            # if train_loss < 0.01:\n",
        "            #     print('Loss is less than 0.01, stopping training')\n",
        "            #     # save the final model\n",
        "            #     self.save_model(f'models/Q1/{self.deep_cnn}_final.pth')\n",
        "            #     break\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        test_accuracy = correct / total\n",
        "        test_loss = running_loss / len(test_loader)\n",
        "        return test_loss, test_accuracy\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        test_loss, test_accuracy = self.evaluate(test_loader)\n",
        "        print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    def plot_cm_train(self, train_loader):\n",
        "        self.model.eval()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = self.model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(predicted.cpu().numpy())\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        annot_labels = np.array([[f'{norm*100:.2f}%' for norm in row] for row in cm_norm])\n",
        "        sns.heatmap(cm, annot=annot_labels, fmt='', xticklabels=train_loader.dataset.classes, yticklabels=train_loader.dataset.classes, cmap='YlGnBu')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix (Train)')\n",
        "        plt.savefig(f'plots/confusion_matrix_train_{self.deep_cnn}.pdf', format='pdf', bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_cm_test(self, test_loader):\n",
        "        self.model.eval()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = self.model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(predicted.cpu().numpy())\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        annot_labels = np.array([[f'{norm*100:.2f}%' for norm in row] for row in cm_norm])\n",
        "        sns.heatmap(cm, annot=annot_labels, fmt='', xticklabels=test_loader.dataset.classes, yticklabels=test_loader.dataset.classes, cmap='YlGnBu')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix (Test)')\n",
        "        plt.savefig(f'plots/confusion_matrix_test_{self.deep_cnn}.pdf', format='pdf', bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Train Loss')\n",
        "        plt.savefig(f'plots/losses_{self.deep_cnn}.pdf', format='pdf', bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_accuracies(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.train_accuracies, label='Train Accuracy')\n",
        "\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Train Accuracy')\n",
        "        plt.savefig(f'plots/accuracies_{self.deep_cnn}.pdf', format='pdf', bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "\n"
      ],
      "metadata": {
        "id": "AqbhRQrEBObt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGGNet setup for Feature Extraction"
      ],
      "metadata": {
        "id": "b42SUIbMCUH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGGNet (without fully connected layers)\n",
        "vgg = models.vgg16(pretrained=True).features\n",
        "# vgg = vgg.features  # Remove the classifier (fully connected layers)\n",
        "vgg = vgg.to(device)\n",
        "vgg.eval()\n",
        "\n",
        "# Freeze the VGGNet parameters\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Print the VGGNet architecture\n",
        "print(vgg)\n",
        "\n",
        "# Define MLFFNN architecture using VGGNet features\n",
        "class MLFFNN_VGG(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLFFNN_VGG, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = vgg(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the feature maps\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9AMSfQohBS-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluaton using VGGNet features"
      ],
      "metadata": {
        "id": "5VSMhtqPCm_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "hidden_size = 256\n",
        "num_classes = len(train_dataset.classes) # = 5\n",
        "model = MLFFNN_VGG(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train and test the model\n",
        "image_classification_vgg = ImageClassification(model, criterion, optimizer, deep_cnn='vgg')\n",
        "image_classification_vgg.train(train_loader, num_epochs=10)\n",
        "image_classification_vgg.test(test_loader)\n",
        "\n",
        "image_classification_vgg.plot_cm_train(train_loader)\n",
        "image_classification_vgg.plot_cm_test(test_loader)\n",
        "image_classification_vgg.plot_losses()\n",
        "image_classification_vgg.plot_accuracies()"
      ],
      "metadata": {
        "id": "xFkF3eAQBdK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GoogLeNet setup for Feature Extraction"
      ],
      "metadata": {
        "id": "TPiVx7H7Cxkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GoogLeNet\n",
        "googlenet = models.googlenet(pretrained=True)\n",
        "googlenet = googlenet.to(device)\n",
        "googlenet.eval()\n",
        "googlenet_features = nn.Sequential(*list(googlenet.children())[:-2])  # exclude FC + aux classifiers\n",
        "\n",
        "# Freeze the GoogLeNet parameters\n",
        "for param in googlenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Print the GoogLeNet architecture\n",
        "print(googlenet)\n",
        "\n",
        "# MLFFNN architecture using GoogLeNet features\n",
        "class MLFFNN_GoogLeNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLFFNN_GoogLeNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = googlenet_features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "DF-ZOb-jBZgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation using GoogleNet Features"
      ],
      "metadata": {
        "id": "mOD_uc5xC3iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "# input_size = 1024  # GoogLeNet output size\n",
        "hidden_size = 256\n",
        "num_classes = len(train_dataset.classes) # = 5\n",
        "model = MLFFNN_GoogLeNet(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Train and test the model\n",
        "image_classification_googlenet = ImageClassification(model, criterion, optimizer, deep_cnn='googlenet')\n",
        "image_classification_googlenet.train(train_loader, num_epochs=10)\n",
        "image_classification_googlenet.test(test_loader)\n",
        "\n",
        "\n",
        "# Plot confusion matrix, losses, and accuracies\n",
        "image_classification_googlenet.plot_cm_train(train_loader)\n",
        "image_classification_googlenet.plot_cm_test(test_loader)\n",
        "image_classification_googlenet.plot_losses()\n",
        "image_classification_googlenet.plot_accuracies()\n"
      ],
      "metadata": {
        "id": "G8-Yku37Bj4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}