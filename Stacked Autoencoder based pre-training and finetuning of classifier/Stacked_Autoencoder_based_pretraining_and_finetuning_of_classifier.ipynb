{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ze_QYuuY1Lu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPXIQr3hzJ2h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Datasets\n",
        "\n",
        "- Replace the path while using the code\n",
        "\n"
      ],
      "metadata": {
        "id": "n3eYHXwX1Osv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "unlabeled = np.genfromtxt(\"/content/drive/MyDrive/CS6910/Assignment1/task3/training_data_set_17_unlabeled.csv\", delimiter=',')\n",
        "unlabeled = torch.tensor(unlabeled,dtype =torch.float32)\n",
        "unlabeled_train = dataset(unlabeled,unlabeled)\n",
        "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_train,batch_size = int(len(unlabeled)*0.1),shuffle = True)\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(path1,path2):\n",
        "    labeled = np.genfromtxt(path1, delimiter=',')\n",
        "    label=np.genfromtxt(path2, delimiter=',')\n",
        "    labeled_label = torch.nn.functional.one_hot(torch.tensor(label,dtype = torch.long),num_classes = -1)\n",
        "\n",
        "    labeled_train = dataset(labeled,labeled_label)\n",
        "    labeled_loader = torch.utils.data.DataLoader(labeled_train,batch_size = int(len(labeled)*0.1),shuffle = True)\n",
        "    return labeled_loader, labeled, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_labeled_loader, train_labeled, train_label= load_dataset(path1 = \"/content/drive/MyDrive/CS6910/Assignment1/task3/training_data_set_17_labeled_data.csv\",path2='/content/drive/MyDrive/CS6910/Assignment1/task3/training_data_set_17_labeled_labels.csv')\n",
        "val_labeled_loader, val_labeled,val_label = load_dataset(path1 = \"/content/drive/MyDrive/CS6910/Assignment1/task3/validation_data_set_17_data.csv\",path2='/content/drive/MyDrive/CS6910/Assignment1/task3/validation_data_set_17_labels.csv')\n",
        "test_labeled_loader, test_labeled,test_label = load_dataset(path1 = \"/content/drive/MyDrive/CS6910/Assignment1/task3/testing_data_set_17_data.csv\",path2='/content/drive/MyDrive/CS6910/Assignment1/task3/testing_data_set_17_labels.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_labeled = '/content/drive/MyDrive/CS6910/Assignment1/task3/training_data_set_17_labeled_data.csv'\n",
        "y_train_labeled = '/content/drive/MyDrive/CS6910/Assignment1/task3/training_data_set_17_labeled_labels.csv'\n",
        "\n",
        "X_train_labeled_df = pd.read_csv(X_train_labeled, header=None)\n",
        "y_train_labeled_df = pd.read_csv(y_train_labeled, header=None)\n",
        "\n",
        "\n",
        "X_train_labeled_tensor = torch.tensor(X_train_labeled_df.values, dtype=torch.float32)\n",
        "y_train_labeled_tensor = torch.tensor(y_train_labeled_df.values, dtype=torch.float32)\n",
        "y_train_labeled_tensor = torch.nn.functional.one_hot(y_train_labeled_tensor.squeeze().long(), num_classes=5).float()\n",
        "\n",
        "dataset_labeled = dataset(X_train_labeled_tensor, y_train_labeled_tensor)\n",
        "train_labeled= np.genfromtxt('/content/drive/MyDrive/CS6910/Assignment1/task3/training_data_set_17_labeled_data.csv', delimiter=',')\n",
        "train_labeled_loader = torch.utils.data.DataLoader(dataset_labeled, batch_size=len(dataset_labeled) // 10, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_val_labeled = '/content/drive/MyDrive/CS6910/Assignment1/task3/validation_data_set_17_data.csv'\n",
        "y_val_labeled = '/content/drive/MyDrive/CS6910/Assignment1/task3/validation_data_set_17_labels.csv'\n",
        "\n",
        "X_val_labeled_df = pd.read_csv(X_val_labeled, header=None)\n",
        "y_val_labeled_df = pd.read_csv(y_val_labeled, header=None)\n",
        "\n",
        "\n",
        "X_val_labeled_tensor = torch.tensor(X_val_labeled_df.values, dtype=torch.float32)\n",
        "y_val_labeled_tensor = torch.tensor(y_val_labeled_df.values, dtype=torch.float32)\n",
        "y_val_labeled_tensor = torch.nn.functional.one_hot(y_val_labeled_tensor.squeeze().long(), num_classes=5).float()\n",
        "\n",
        "val_labeled=np.genfromtxt('/content/drive/MyDrive/CS6910/Assignment1/task3/validation_data_set_17_data.csv', delimiter=',')\n",
        "dataset_val = dataset(X_val_labeled_tensor, y_val_labeled_tensor)\n",
        "\n",
        "val_labeled_loader = torch.utils.data.DataLoader(dataset_val, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_test_labeled = '/content/drive/MyDrive/CS6910/Assignment1/task3/testing_data_set_17_data.csv'\n",
        "y_test_labeled = '/content/drive/MyDrive/CS6910/Assignment1/task3/testing_data_set_17_labels.csv'\n",
        "\n",
        "X_test_labeled_df = pd.read_csv(X_test_labeled, header=None)\n",
        "y_test_labeled_df = pd.read_csv(y_test_labeled, header=None)\n",
        "\n",
        "\n",
        "X_test_labeled_tensor = torch.tensor(X_test_labeled_df.values, dtype=torch.float32)\n",
        "y_test_labeled_tensor = torch.tensor(y_test_labeled_df.values, dtype=torch.float32)\n",
        "y_test_labeled_tensor = torch.nn.functional.one_hot(y_test_labeled_tensor.squeeze().long(), num_classes=5).float()\n",
        "\n",
        "test_labeled=np.genfromtxt('/content/drive/MyDrive/CS6910/Assignment1/task3/testing_data_set_17_data.csv', delimiter=',')\n",
        "\n",
        "dataset_test = dataset(X_test_labeled_tensor, y_test_labeled_tensor)\n",
        "\n",
        "test_labeled_loader = torch.utils.data.DataLoader(dataset_test, batch_size=len(dataset_test) // 10, shuffle=True)\n"
      ],
      "metadata": {
        "id": "cKwt1dFjzMg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder Definition"
      ],
      "metadata": {
        "id": "pQLlfMPe1Wg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AANN(nn.Module):\n",
        "  def __init__(self,inputnodes,hlnodes,compressednodes):#nodelist : list of number of nodes till bottleneck layer (including bottleneck layer) e.g [30,20,10] in a 5 layer AANN\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(nn.Linear(inputnodes,hlnodes),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(hlnodes,compressednodes))\n",
        "\n",
        "    self.decoder = nn.Sequential(nn.Linear(compressednodes,hlnodes),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(hlnodes,inputnodes))\n",
        "\n",
        "  def forward(self,x):\n",
        "    compressed = self.encoder(x)\n",
        "    out = self.decoder(compressed)\n",
        "    return out, compressed\n",
        "\n",
        "\n",
        "class StackedAutoEncoders(nn.Module):\n",
        "  def __init__(self,aann1,aann2,aann3,compressed_size, outnodes):\n",
        "    super().__init__()\n",
        "    self.aann1 = aann1\n",
        "    self.aann2 = aann2\n",
        "    self.aann3 = aann3\n",
        "    self.outlayer = nn.Linear(compressed_size, outnodes)\n",
        "\n",
        "  def forward(self,x):\n",
        "      _, x = self.aann1(x)\n",
        "      _, x = self.aann2(x)\n",
        "      _, x = self.aann3(x)\n",
        "\n",
        "      return nn.functional.softmax(self.outlayer(x),dim=1)"
      ],
      "metadata": {
        "id": "EmMb6H1Wzp0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoder training Helper functions"
      ],
      "metadata": {
        "id": "azn_60KN1a36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_AE(Aann1, Aann2, Aann3, unlabeled_loader, unlabeled, threshold = 1e-5,learning_rate = 0.0003):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer1 = torch.optim.Adam(Aann1.parameters(), lr = learning_rate)\n",
        "    optimizer2 = torch.optim.Adam(Aann2.parameters(), lr = learning_rate)\n",
        "    optimizer3 = torch.optim.Adam(Aann3.parameters(), lr = learning_rate)\n",
        "\n",
        "    err = np.inf\n",
        "    loss_list  = []\n",
        "    print(\"-\"*30+\"\\n Traning 1st AANN\\n\"+\"-\"*30)\n",
        "    while err > threshold:\n",
        "          loss=0\n",
        "          for input1, output1 in unlabeled_loader:\n",
        "            optimizer1.zero_grad()\n",
        "            out1, _ = Aann1(input1)\n",
        "            loss1 = criterion(out1, output1.float())\n",
        "            loss += loss1*len(input1)\n",
        "            loss1.backward()\n",
        "            optimizer1.step()\n",
        "\n",
        "          loss/=len(unlabeled)\n",
        "          if len(loss_list)>0:\n",
        "            err = abs(loss - loss_list[-1])\n",
        "          loss_list.append(loss)\n",
        "          if not(len(loss_list)%50) or err < threshold:\n",
        "            print(\"Epoch{} Train loss : {:.4f} \".format(len(loss_list), loss.item()))\n",
        "\n",
        "\n",
        "    err = np.inf\n",
        "    loss_list  = []\n",
        "    print(\"-\"*30+\"\\n Traning 2nd AANN\\n\"+\"-\"*30)\n",
        "    while err > threshold:\n",
        "          loss = 0\n",
        "          for input1, output1 in unlabeled_loader:\n",
        "            optimizer2.zero_grad()\n",
        "            with torch.no_grad():\n",
        "              _, comp1 = Aann1(input1)\n",
        "\n",
        "            out2, _ = Aann2(comp1)\n",
        "\n",
        "            loss2 = criterion(out2, comp1)\n",
        "            loss += loss2*len(input1)\n",
        "            loss2.backward()\n",
        "            optimizer2.step()\n",
        "\n",
        "          loss/=len(unlabeled)\n",
        "          if len(loss_list)>0:\n",
        "            err = abs(loss - loss_list[-1])\n",
        "          loss_list.append(loss)\n",
        "          if not(len(loss_list)%50) or err < threshold:\n",
        "              print(\"Epoch{} Train loss : {:.4f} \".format(len(loss_list), loss.item()))\n",
        "\n",
        "\n",
        "    err = np.inf\n",
        "    loss_list  = []\n",
        "    print(\"-\"*30+\"\\n Traning 3rd AANN\\n\"+\"-\"*30)\n",
        "    while err > threshold:\n",
        "          loss = 0\n",
        "          for input1, output1 in unlabeled_loader:\n",
        "            optimizer3.zero_grad()\n",
        "            with torch.no_grad():\n",
        "              _, comp1 = Aann1(input1)\n",
        "              _, comp2 = Aann2(comp1)\n",
        "\n",
        "            out3, _ = Aann3(comp2)\n",
        "\n",
        "            loss3 = criterion(out3, comp2)\n",
        "            loss += loss3*len(input1)\n",
        "            loss3.backward()\n",
        "            optimizer3.step()\n",
        "\n",
        "          loss/=len(unlabeled)\n",
        "          if len(loss_list)>0:\n",
        "            err = abs(loss - loss_list[-1])\n",
        "          loss_list.append(loss)\n",
        "          if not(len(loss_list)%50) or err < threshold:\n",
        "            print(\"Epoch{} Train loss : {:.4f} \".format(len(loss_list), loss.item()))\n",
        "\n",
        "    return Aann1, Aann2, Aann3\n",
        "\n",
        "def train_stacked_encoders(StackedAE, train_labeled_loader, train_labeled, val_labeled_loader, val_labeled, learning_rate = 0.0003,threshold = 1e-6):\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer_stacked = torch.optim.Adam(StackedAE.parameters(), lr = learning_rate)\n",
        "      err = np.inf\n",
        "      train_loss = []\n",
        "      train_acc = []\n",
        "      val_acc = []\n",
        "      val_loss = []\n",
        "      print(\"-\"*50+\"\\nFine Tuning Stacked Auto Encoder\\n\"+\"-\"*50)\n",
        "      while err > threshold:\n",
        "          total_loss = 0\n",
        "          total_acc = 0\n",
        "          for img, label in train_labeled_loader:\n",
        "              optimizer_stacked.zero_grad()\n",
        "              pred = StackedAE(img.float())\n",
        "              loss  = criterion(pred, torch.argmax(label.float(), dim =1))\n",
        "              total_loss += loss*len(img)\n",
        "              total_acc += torch.sum(torch.argmax(pred, dim = 1) == torch.argmax(label.float(), dim =1))\n",
        "              loss.backward()\n",
        "              optimizer_stacked.step()\n",
        "\n",
        "          train_acc.append(total_acc.item()/len(train_labeled)*100)\n",
        "          train_loss.append(total_loss.item()/len(train_labeled))\n",
        "          if len(train_loss) > 1:\n",
        "            err = abs(train_loss[-1] - train_loss[-2])\n",
        "          with torch.no_grad():\n",
        "              for img, label in val_labeled_loader :\n",
        "                  out = StackedAE(img.float())\n",
        "              acc = torch.sum((torch.argmax(out, dim = 1) == torch.argmax(label.float(), dim = 1)))/len(label)\n",
        "              loss = criterion(out, torch.argmax(label.float(), dim =1))\n",
        "              val_acc.append(acc.item()*100)\n",
        "              val_loss.append(loss.item())\n",
        "\n",
        "          if not(len(train_loss)%50) or err < threshold:\n",
        "            print(\"Epoch{} Train Acc : {:.4f} Train loss : {:.4f} Val Acc : {:.4f} Val Loss : {:.4f}\".format(len(train_loss),\n",
        "                                                                              train_acc[-1], train_loss[-1], acc.item()*100, loss.item()))\n",
        "\n",
        "      return StackedAE\n",
        "\n",
        "def update_dataset(StackedAE, unlabeled, train_labeled,train_label, prob_threshold = 0.5):\n",
        "    no_unlabeled_dataset = False\n",
        "    with torch.no_grad():\n",
        "        pred = StackedAE(unlabeled)\n",
        "    max_values, unlabeled_classes  = torch.max(pred, axis = 1)\n",
        "    indices = torch.where(torch.sum(max_values.unsqueeze(dim=1)-pred, axis = 1)/4>prob_threshold)[0]\n",
        "\n",
        "\n",
        "    updated_train_labeled = np.vstack([train_labeled,unlabeled[indices]])\n",
        "    updated_train_label=np.vstack([train_label.reshape(-1,1),unlabeled_classes[indices].unsqueeze(dim=1)])\n",
        "    unlabeled_indices = np.array(list(set(range(len(unlabeled)))-set(indices.numpy())))\n",
        "    if len(unlabeled_indices) == 0:\n",
        "        no_unlabeled_dataset = True\n",
        "    updated_unlabeled = unlabeled[unlabeled_indices]\n",
        "    print(\"-\"*60+f\"\\nSucessfully labeled {len(indices)} and added to training data\\n\"+\"-\"*60)\n",
        "    return no_unlabeled_dataset, updated_unlabeled, updated_train_labeled , updated_train_label.reshape(-1)\n",
        "\n",
        "\n",
        "def train_stacked(unlabeled_loader, unlabeled, train_labeled_loader, train_labeled,train_label, val_labeled_loader, val_labeled,val_label):\n",
        "    compressed_size = 6\n",
        "    outnodes = len(np.unique(train_label))\n",
        "\n",
        "    Aann1 = AANN(36,30,26)\n",
        "    Aann2 = AANN(26,20,16)\n",
        "    Aann3 = AANN(16,10,6)\n",
        "\n",
        "\n",
        "    set_seed()\n",
        "    init_weights(Aann1)\n",
        "    init_weights(Aann2)\n",
        "    init_weights(Aann3)\n",
        "\n",
        "    StackedAE = StackedAutoEncoders(Aann1,Aann2,Aann3, compressed_size, outnodes)\n",
        "\n",
        "\n",
        "    Aann1, Aann2, Aann3 = train_AE(Aann1, Aann2, Aann3, unlabeled_loader, unlabeled)\n",
        "    StackedAE = train_stacked_encoders(StackedAE, train_labeled_loader, train_labeled, val_labeled_loader, val_labeled)\n",
        "\n",
        "    return StackedAE\n",
        "\n",
        "def plot_confusion_matrix(model, data, label):\n",
        "    with torch.no_grad() :\n",
        "        out = model(data)\n",
        "\n",
        "    confusion_matrix = metrics.confusion_matrix(torch.tensor(label), torch.argmax(out, dim= 1))\n",
        "\n",
        "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
        "\n",
        "    cm_display.plot()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pm3QtcElz3It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep FeedForward Network definition and train function\n"
      ],
      "metadata": {
        "id": "_exy4zZx1hCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DFNN(nn.Module):\n",
        "    def __init__(self, input_size, out_nodes):\n",
        "      super().__init__()\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Linear(input_size, 26), nn.Tanh(),\n",
        "          nn.Linear(26, 16), nn.Tanh(),\n",
        "          nn.Linear(16, 6), nn.Tanh(),\n",
        "          nn.Linear(6, out_nodes), nn.Softmax(dim=1)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.layers(x)\n",
        "\n",
        "\n",
        "def train_dfnn(dfnn, learning_rate = 0.0003,  threshold = 1e-6):\n",
        "    #init_weights(dfnn)     #Comment this line out when training a pre-trained model\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(dfnn.parameters(), lr = learning_rate)\n",
        "\n",
        "    err = np.inf\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    val_loss = []\n",
        "    while err > threshold:\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for img, label in train_labeled_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = dfnn(img.float())\n",
        "            loss  = criterion(pred, label.float())\n",
        "            total_loss += loss*len(img)\n",
        "            total_acc += torch.sum(torch.argmax(pred, dim = 1) == torch.argmax(label.float(), dim =1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_acc.append(total_acc.item()/len(train_labeled)*100)\n",
        "        train_loss.append(total_loss.item()/len(train_labeled))\n",
        "        if len(train_loss) > 1:\n",
        "          err = abs(train_loss[-1] - train_loss[-2])\n",
        "        with torch.no_grad():\n",
        "            for img, label in val_labeled_loader :\n",
        "                out = dfnn(img.float())\n",
        "            acc = torch.sum((torch.argmax(out, dim = 1) == torch.argmax(label.float(), dim = 1)))/len(label)\n",
        "            loss = criterion(out, label.float())\n",
        "            val_acc.append(acc.item()*100)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "\n",
        "        print(\"Epoch{} Train Acc : {:.4f} Train loss : {:.4f} Val Acc : {:.4f} Val Loss : {:.4f}\".format(len(train_loss),\n",
        "                                                                            train_acc[-1], train_loss[-1], acc.item()*100, loss.item()))\n",
        "\n",
        "\n",
        "    return dfnn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lG4CTNAb0Lwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training AANN"
      ],
      "metadata": {
        "id": "0ongYuAJ1rRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StackedAE = train_stacked(unlabeled_loader, unlabeled, train_labeled_loader, train_labeled,train_label, val_labeled_loader, val_labeled,val_label)\n",
        "plot_confusion_matrix(StackedAE, torch.tensor(train_labeled, dtype = torch.float32),torch.nn.functional.one_hot(torch.tensor(train_label,dtype=torch.long),num_classes=-1) )\n",
        "plot_confusion_matrix(StackedAE, torch.tensor(test_labeled, dtype = torch.float32),torch.nn.functional.one_hot(torch.tensor(test_label,dtype=torch.long),num_classes=-1))"
      ],
      "metadata": {
        "id": "iQEoQ6Na0oD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training DFNN without Pretraining"
      ],
      "metadata": {
        "id": "-WGCp_i71vay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outnodes = len(np.unique(train_label))\n",
        "dfnn = DFNN(unlabeled.shape[1], outnodes)#after training AEs\n",
        "dfnn = train_dfnn(dfnn)\n",
        "plot_confusion_matrix(dfnn, torch.tensor(test_labeled, dtype = torch.float32), torch.nn.functional.one_hot(torch.tensor(test_label,dtype=torch.long),num_classes=-1))"
      ],
      "metadata": {
        "id": "uBVU1qmc0eP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training DFNN with Pretraining"
      ],
      "metadata": {
        "id": "DT-rGhov1yy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class p_DFNN(nn.Module):\n",
        "    def __init__(self, stacked_autoencoder,outnodes):\n",
        "        super().__init__()\n",
        "        # Load pretrained encoder layers\n",
        "        self.encoder1 = stacked_autoencoder.aann1\n",
        "        self.encoder2 = stacked_autoencoder.aann2\n",
        "        self.encoder3 = stacked_autoencoder.aann3\n",
        "\n",
        "        # Fully connected classifier head\n",
        "        self.fc = nn.Linear(6, outnodes)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        _,x = self.encoder1(x)  # Output of AANN1\n",
        "        _,x = self.encoder2(x)  # Output of AANN2\n",
        "        _,x = self.encoder3(x)  # Output of AANN3\n",
        "        return nn.functional.softmax(self.fc(x), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "outnodes = len(np.unique(train_label))\n",
        "p_dfnn = p_DFNN(StackedAE,outnodes)#after training AEs\n",
        "p_dfnn = train_dfnn(p_dfnn)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(p_dfnn, torch.tensor(train_labeled, dtype = torch.float32), torch.nn.functional.one_hot(torch.tensor(train_label,dtype=torch.long),num_classes=-1))\n",
        "plot_confusion_matrix(p_dfnn, torch.tensor(test_labeled, dtype = torch.float32), torch.nn.functional.one_hot(torch.tensor(test_label,dtype=torch.long),num_classes=-1))"
      ],
      "metadata": {
        "id": "SrujABEU0sBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}