{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "dz4Gaffkp194"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wq7f3R0noXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Datasets\n",
        "\n",
        "- Replace the path while using the code\n"
      ],
      "metadata": {
        "id": "-Gr0Wt-Cp7Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "train_data = np.genfromtxt(\"/content/drive/MyDrive/CS6910/FoDL/task1/train_data.csv\", delimiter=',')\n",
        "train_label = np.genfromtxt(\"/content/drive/MyDrive/CS6910/FoDL/task1/train_label.csv\", delimiter=',')\n",
        "val_data = np.genfromtxt(\"/content/drive/MyDrive/CS6910/FoDL/task1/val_data.csv\", delimiter=',')\n",
        "val_label = np.genfromtxt(\"/content/drive/MyDrive/CS6910/FoDL/task1/val_label.csv\", delimiter=',')\n",
        "test_data = np.genfromtxt(\"/content/drive/MyDrive/CS6910/FoDL/task1/test_data.csv\", delimiter=',')\n",
        "test_label = np.genfromtxt(\"/content/drive/MyDrive/CS6910/FoDL/task1/test_label.csv\", delimiter=',')\n",
        "\n",
        "\n",
        "train_dataset = torch.tensor(train_data,dtype = torch.float32)\n",
        "train_label = torch.nn.functional.one_hot(torch.tensor(train_label,dtype = torch.long),num_classes = -1)\n",
        "val_dataset = torch.tensor(val_data,dtype = torch.float32)\n",
        "val_label = torch.nn.functional.one_hot(torch.tensor(val_label,dtype = torch.long),num_classes = -1)\n",
        "test_dataset = torch.tensor(test_data,dtype = torch.float32)\n",
        "test_label = torch.nn.functional.one_hot(torch.tensor(test_label,dtype = torch.long),num_classes = -1)"
      ],
      "metadata": {
        "id": "XbgkkCUsoIUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definitions and helper functions"
      ],
      "metadata": {
        "id": "lN7UUJoTqADY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,data,label):\n",
        "    self.data = data\n",
        "    self.label = label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.data[idx],self.label[idx]\n",
        "\n",
        "\n",
        "\n",
        "class MLFFNN(nn.Module):\n",
        "  def __init__(self, batch_norm = True):\n",
        "    super().__init__()\n",
        "    if batch_norm :\n",
        "      self.model = nn.Sequential(nn.Linear(36,60), nn.Tanh(), nn.BatchNorm1d(60),\n",
        "                        nn.Linear(60,30), nn.Tanh(), nn.BatchNorm1d(30),\n",
        "                        nn.Linear(30,5), nn.Softmax(dim = 1)\n",
        "    )\n",
        "    else :\n",
        "      self.model = nn.Sequential(nn.Linear(36,60), nn.Tanh(),\n",
        "                        nn.Linear(60,30), nn.Tanh(),\n",
        "                        nn.Linear(30,5), nn.Softmax(dim = 1)\n",
        "      )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "    if isinstance(m, nn.BatchNorm1d) :\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "def get_accuracy(true, pred):\n",
        "    return torch.sum(torch.argmax(true, axis = 1) == torch.argmax(pred, axis=  1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(train_data, val_data, optim : str, seed: int = 42, batch_norm :bool = False, batch_size = 1, lr = 0.0003, threshold= 1e-5):\n",
        "    optim = optim.lower()\n",
        "    assert optim in [\"delta rule\", \"generalized delta rule\", \"adagrad\", \"rmsprop\", \"adam\"]\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,shuffle = True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data,batch_size = len(val_data),shuffle = True)\n",
        "\n",
        "    learning_rate = lr\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    threshold = threshold #placeholder value\n",
        "\n",
        "    # hidden_layer_neurons = [256, 64]\n",
        "\n",
        "    model = MLFFNN(batch_norm = batch_norm)\n",
        "    seed = 42\n",
        "    set_seed(seed)\n",
        "    model.apply(init_weights)\n",
        "    if optim == \"delta rule\":\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    elif optim == \"generalized delta rule\" :\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
        "    elif optim == \"adagrad\" :\n",
        "      optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate)\n",
        "    elif optim == \"rmsprop\" :\n",
        "      optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
        "    elif optim == \"adam\" :\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    train_acc = []\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    val_acc  = []\n",
        "    err = np.inf\n",
        "    epoch = 0\n",
        "    # num_epochs = 500\n",
        "\n",
        "\n",
        "    # for epoch in range(num_epochs) :\n",
        "    while err>threshold :\n",
        "        total_acc = 0\n",
        "        total_loss = 0\n",
        "        for data in train_loader :\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data[0].float())\n",
        "            loss = criterion(out, data[1].float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_acc += torch.sum(torch.argmax(out, dim = 1) == torch.argmax(data[1], dim =1))\n",
        "            total_loss += loss*len(data[0])\n",
        "\n",
        "        train_acc.append(total_acc.item()/len(train_data)*100)\n",
        "        train_loss.append(total_loss.item()/len(train_data))\n",
        "        if epoch > 1:\n",
        "          err = abs(train_loss[-1] - train_loss[-2])\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader :\n",
        "                out = model(data[0].float())\n",
        "            ######## val\n",
        "            acc = torch.sum((torch.argmax(out, dim = 1) == torch.argmax(data[1], dim = 1)))/len(data[1])\n",
        "            loss = criterion(out, data[1].float())\n",
        "            val_acc.append(acc.item()*100)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        print(\"Epoch{} Train Acc : {:.4f} Train loss : {:.4f} Val Acc : {:.4f} Val Loss : {:.4f}\".format(epoch+1,\n",
        "                                                                        train_acc[-1], train_loss[-1], acc.item()*100, loss.item()))\n",
        "        epoch+=1\n",
        "\n",
        "\n",
        "    plt.figure(figsize= (10,14))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(np.arange(len(train_acc)), train_acc)\n",
        "    plt.plot(np.arange(len(val_acc)), val_acc)\n",
        "    plt.legend([\"Train\", \"Validation\"])\n",
        "    plt.title(\"Epoch vs Accuracy\")\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(np.arange(len(train_loss)), train_loss)\n",
        "    plt.plot(np.arange(len(val_loss)), val_loss)\n",
        "    plt.legend([\"Train\", \"Validation\"])\n",
        "    plt.title(\"Epoch vs Loss\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model, train_loss, val_loss, train_acc, val_acc\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(model, data, label):\n",
        "    with torch.no_grad() :\n",
        "        out = model(data)\n",
        "\n",
        "    confusion_matrix = metrics.confusion_matrix(torch.argmax(label, dim=1), torch.argmax(out, dim= 1))\n",
        "\n",
        "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
        "\n",
        "    cm_display.plot()\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(model, train_dataset, train_label)"
      ],
      "metadata": {
        "id": "v72xVrW-oOvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing datasets and models"
      ],
      "metadata": {
        "id": "plwqESeEqFUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset(train_data,train_label)\n",
        "val_data = dataset(val_data,val_label)\n",
        "test_data = dataset(test_data,test_label)\n",
        "\n",
        "model = MLFFNN()\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "_oPTEiDuor-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Rule"
      ],
      "metadata": {
        "id": "JZjxLOSXqLdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, val_loss, train_acc, val_acc = train(train_data, val_data, batch_norm = False, optim = \"Delta rule\")\n",
        "plot_confusion_matrix(model, test_dataset, test_label)"
      ],
      "metadata": {
        "id": "-aLnwRMfpEBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Delta Rule"
      ],
      "metadata": {
        "id": "nr7siyJoqOEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, val_loss, train_acc, val_acc = train(train_data, val_data, batch_norm = False, optim = \"Generalized Delta rule\")\n",
        "plot_confusion_matrix(model, train_dataset, train_label)\n",
        "plot_confusion_matrix(model, test_dataset, test_label)"
      ],
      "metadata": {
        "id": "wbuk_UMopIRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaGrad"
      ],
      "metadata": {
        "id": "-mSnr3IsqT0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, val_loss, train_acc, val_acc = train(train_data, val_data, batch_norm = False, optim = \"AdaGrad\", lr = 0.0003)\n",
        "plot_confusion_matrix(model, train_dataset, train_label)\n",
        "plot_confusion_matrix(model, test_dataset, test_label)"
      ],
      "metadata": {
        "id": "UQ5tIu5qpOOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp"
      ],
      "metadata": {
        "id": "NecAFjJ5qV9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, val_loss, train_acc, val_acc = train(train_data, val_data, batch_norm = False, optim = \"RMSProp\")\n",
        "plot_confusion_matrix(model, train_dataset, train_label)\n",
        "plot_confusion_matrix(model, test_dataset, test_label)"
      ],
      "metadata": {
        "id": "Q2vS-gzUpPoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adam"
      ],
      "metadata": {
        "id": "_RXGyeR1qX3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, val_loss, train_acc, val_acc = train(train_data, val_data, batch_norm = False, optim = \"Adam\")\n",
        "plot_confusion_matrix(model, train_dataset, train_label)\n",
        "plot_confusion_matrix(model, test_dataset, test_label)"
      ],
      "metadata": {
        "id": "beMJLfiZpY6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}