{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "qpdVjK-pb6Bn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OrfD4lrRYXPL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "C9Dl4aAPcD3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_SIZE = 50\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "glove_path = \"/content/glove.6B.200d.txt\"\n"
      ],
      "metadata": {
        "id": "vnRN6IxwYh7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Helper functions\n"
      ],
      "metadata": {
        "id": "NLi1BxtdcHMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_dataset(dataset, split_ratio=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Splits a dataset into train and test sets.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The full dataset to split.\n",
        "        split_ratio (float): Ratio for train set. (0.8 means 80% train, 20% test)\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        train_dataset (Subset), test_dataset (Subset)\n",
        "    \"\"\"\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    split = int(split_ratio * dataset_size)\n",
        "    train_indices = indices[:split]\n",
        "    test_indices = indices[split:]\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_folder, image_names_file, captions_file,vocab,transform=None,max_caption_length=30):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "\n",
        "        # Load image names\n",
        "        with open(image_names_file, 'r') as f:\n",
        "            image_names = [line.strip() for line in f]\n",
        "\n",
        "        # Load captions and group by image name (excluding #index)\n",
        "        self.captions_dict = {}\n",
        "        with open(captions_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "                img_full_name, caption = parts\n",
        "                img_name = img_full_name.split('#')[0]\n",
        "                if img_name not in self.captions_dict:\n",
        "                    self.captions_dict[img_name] = []\n",
        "                self.captions_dict[img_name].append(caption)\n",
        "\n",
        "        # Filter image names that are present in captions and exist in the folder\n",
        "        self.valid_image_names = [\n",
        "            name for name in image_names\n",
        "            if name in self.captions_dict and os.path.exists(os.path.join(image_folder, name))\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.valid_image_names[idx]\n",
        "        img_path = os.path.join(self.image_folder, img_name)\n",
        "\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"[WARNING] Skipping missing image: {img_path}\")\n",
        "            return None  # Dataset returns None if image is missing\n",
        "\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Use the first caption by default (you can change this)\n",
        "        caption = self.captions_dict[img_name][0]\n",
        "        caption_tensor = self.vocab.caption_to_tensor(caption, self.max_caption_length)\n",
        "\n",
        "\n",
        "        return image, caption_tensor\n"
      ],
      "metadata": {
        "id": "EHkX2OymYaYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Setup"
      ],
      "metadata": {
        "id": "4TD7FLy3cLTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(glove_path, vocab, embedding_dim=200):\n",
        "    \"\"\"\n",
        "    Loads GloVe embeddings and returns an embedding matrix for the given vocab.\n",
        "\n",
        "    Args:\n",
        "        glove_path (str): Path to glove.6B.200d.txt\n",
        "        vocab (Vocabulary): Vocabulary object\n",
        "        embedding_dim (int): Dimension of GloVe vectors (default 200)\n",
        "\n",
        "    Returns:\n",
        "        embedding_matrix (np.ndarray): Shape (vocab_size, embedding_dim)\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "\n",
        "    # Load glove embeddings\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, idx in vocab.stoi.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "        else:\n",
        "            # Random initialization for words not in GloVe\n",
        "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "RoW0RZX4Yude"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary Setup"
      ],
      "metadata": {
        "id": "O_48JbBJcORZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=1):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = len(self.itos)\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            tokens = sentence.lower().split()\n",
        "            frequencies.update(tokens)\n",
        "\n",
        "        for word, freq in frequencies.items():\n",
        "            if freq >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        return [\n",
        "            self.stoi.get(word, self.stoi[\"<UNK>\"])\n",
        "            for word in text.lower().split()\n",
        "        ]\n",
        "\n",
        "    def caption_to_tensor(self, caption, max_length=None):\n",
        "        numericalized = [self.stoi[\"<SOS>\"]] + self.numericalize(caption) + [self.stoi[\"<EOS>\"]]\n",
        "\n",
        "        if max_length:\n",
        "            if len(numericalized) < max_length:\n",
        "                numericalized += [self.stoi[\"<PAD>\"]] * (max_length - len(numericalized))\n",
        "            else:\n",
        "                numericalized = numericalized[:max_length-1] + [self.stoi[\"<EOS>\"]]\n",
        "\n",
        "        return torch.tensor(numericalized, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function that pads captions and stacks images.\n",
        "    \"\"\"\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=0)\n",
        "    return images, captions_padded\n"
      ],
      "metadata": {
        "id": "VmR6baZ3Ydc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "OQSiaMjhcRZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGEncoder(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(VGGEncoder, self).__init__()\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Use features only (no classifier)\n",
        "        self.vgg_features = vgg.features\n",
        "\n",
        "        # Freeze VGG parameters (optional, for faster training)\n",
        "        for param in self.vgg_features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add adaptive pooling and linear projection\n",
        "        self.fc = nn.Linear(512 * 7 * 7, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.vgg_features(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.fc(features)\n",
        "        features = self.bn(features)\n",
        "        return features  # Shape: (batch_size, embed_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, vocab_size, num_layers=1, freeze_embeddings=False):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix), freeze=freeze_embeddings\n",
        "        )\n",
        "\n",
        "        embed_size = embedding_matrix.shape[1]\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, captions, features):\n",
        "        embeddings = self.embed(captions)  # (B, T, E)\n",
        "\n",
        "        features = features.unsqueeze(1)  # (B, 1, E)\n",
        "        inputs = torch.cat((features, embeddings), dim=1)  # (B, T+1, E)\n",
        "\n",
        "        outputs, _ = self.lstm(inputs)\n",
        "        outputs = self.linear(outputs)  # (B, T+1, vocab_size)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, vocab_size, num_layers=1, freeze_embeddings=False):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding.from_pretrained(\n",
        "            embedding_matrix, freeze=freeze_embeddings\n",
        "        )\n",
        "\n",
        "        embed_size = embedding_matrix.shape[1]\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, captions, features):\n",
        "        embeddings = self.embed(captions)  # (B, T, E)\n",
        "\n",
        "        features = features.unsqueeze(1)  # (B, 1, E)\n",
        "        inputs = torch.cat((features, embeddings), dim=1)  # (B, T+1, E)\n",
        "\n",
        "        outputs, _ = self.rnn(inputs)\n",
        "        outputs = self.linear(outputs)  # (B, T+1, vocab_size)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder, vocab, device):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vocab = vocab\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        # Forward pass for training\n",
        "        features = self.encoder(images)  # (B, E)\n",
        "        outputs = self.decoder(captions[:, :-1], features)  # (B, T, V)\n",
        "        return outputs\n",
        "\n",
        "    def generate_caption(self, image, max_length=15):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode the image\n",
        "            feature = self.encoder(image.unsqueeze(0).to(self.device))  # (1, E)\n",
        "\n",
        "            # Start with <start> token\n",
        "            caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "            input_caption = torch.tensor(caption, dtype=torch.long).to(self.device)\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                # Get embeddings and decoder output\n",
        "                output = self.decoder(input_caption.unsqueeze(0), feature)  # (1, T+1, V)\n",
        "                output = output[0, -1, :]  # Get last timestep output\n",
        "\n",
        "                predicted_idx = output.argmax(dim=-1).item()\n",
        "                caption.append(predicted_idx)\n",
        "\n",
        "                if self.vocab.itos[predicted_idx] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "                input_caption = torch.tensor(caption[1:], dtype=torch.long).to(self.device)\n",
        "\n",
        "            # Convert to words, skip <start> and stop at <end>\n",
        "            words = [self.vocab.itos[idx] for idx in caption[1:]]\n",
        "            final_caption = []\n",
        "            for word in words:\n",
        "                if word == \"<EOS>\":\n",
        "                    break\n",
        "                final_caption.append(word)\n",
        "\n",
        "            return ' '.join(final_caption)\n"
      ],
      "metadata": {
        "id": "sXhRj-ihaLpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train helper"
      ],
      "metadata": {
        "id": "U-Jx8fKqcVLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, criterion, optimizer, vocab, device):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.vocab = vocab\n",
        "        self.device = device\n",
        "\n",
        "        # Track metrics for plotting\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "\n",
        "    def _calculate_accuracy(self, outputs, targets):\n",
        "        _, predicted = outputs.max(2)  # (B, T)\n",
        "        correct = (predicted == targets).float()\n",
        "        mask = (targets != self.vocab.stoi[\"<PAD>\"]).float()\n",
        "        accuracy = (correct * mask).sum() / mask.sum()\n",
        "        return accuracy.item()\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            steps = 0\n",
        "\n",
        "            for images, captions in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                images = images.to(self.device)\n",
        "                captions = captions.to(self.device)\n",
        "\n",
        "                if images is None or captions is None:\n",
        "                    continue\n",
        "\n",
        "                # Prepare decoder input and target\n",
        "                inputs = captions[:, :-1]     # Exclude <end>\n",
        "                targets = captions[:, 1:]     # Exclude <start>\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images, inputs)  # (B, T-1, V)\n",
        "\n",
        "                # Flatten outputs and targets for loss computation\n",
        "                loss = self.criterion(\n",
        "                    outputs.reshape(-1, outputs.size(2)),  # (B*(T-1), V)\n",
        "                    targets.reshape(-1)                    # (B*(T-1))\n",
        "                )\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                acc = self._calculate_accuracy(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "                total_acc += acc\n",
        "                steps += 1\n",
        "\n",
        "            avg_loss = total_loss / steps\n",
        "            avg_acc = total_acc / steps\n",
        "            self.train_losses.append(avg_loss)\n",
        "            self.train_accuracies.append(avg_acc)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {avg_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "xXnX_iHOaNos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "q6PlE1zucaeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Replace 'file_name.zip' with the name of your zip file\n",
        "zip_file_path = '/content/drive/MyDrive/CS6910/Images.zip'\n",
        "\n",
        "extract_to_path = '/content'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/image_names.txt', 'r') as f:\n",
        "    image_names = [line.strip() for line in f]\n",
        "\n",
        "# Load captions and group by image name (excluding #index)\n",
        "captions_dict = {}\n",
        "with open('/content/captions.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        img_full_name, caption = parts\n",
        "        img_name = img_full_name.split('#')[0]\n",
        "        if img_name not in captions_dict:\n",
        "            captions_dict[img_name] = []\n",
        "        captions_dict[img_name].append(caption)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "all_captions = []\n",
        "for caps in captions_dict.values():\n",
        "    all_captions.extend(caps)\n",
        "\n",
        "vocab.build_vocabulary(all_captions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sMw9C31-ajQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = load_glove_embeddings(glove_path, vocab, EMBEDDING_DIM)\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "dataset=ImageCaptionDataset('/content/Images/','/content/image_names.txt','/content/captions.txt',vocab)\n",
        "\n",
        "train_dataset, test_dataset = split_dataset(dataset, split_ratio=0.8)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,collate_fn=collate_fn)  # For BLEU\n"
      ],
      "metadata": {
        "id": "n06xGa62a47z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = VGGEncoder(EMBEDDING_DIM).to(DEVICE)\n",
        "decoder = RNNDecoder(embedding_matrix, HIDDEN_SIZE, len(vocab)).to(DEVICE)\n",
        "\n",
        "\n",
        "model = ImageCaptionModel(encoder, decoder, vocab,DEVICE).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(model, train_loader, criterion, optimizer, vocab, DEVICE)\n",
        "trainer.train(NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(trainer.train_losses, label='Loss')\n",
        "plt.plot(trainer.train_accuracies, label='Accuracy')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss & Accuracy\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mavrc5Qya9zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model BLEU Evaluation"
      ],
      "metadata": {
        "id": "fpMW5BM7citR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(model, dataloader, vocab, device, max_samples=1000):\n",
        "    model.eval()\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    total_bleu1 = 0\n",
        "    total_bleu2 = 0\n",
        "    total_bleu3 = 0\n",
        "    total_bleu4 = 0\n",
        "\n",
        "    num_samples = min(len(dataloader.dataset), max_samples)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, captions) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
        "            if i * images.size(0) >= max_samples:\n",
        "                break\n",
        "\n",
        "            for img, true_caption in zip(images, captions):\n",
        "                img = img.to(device)\n",
        "\n",
        "                # Generate prediction\n",
        "                pred = model.generate_caption(img)\n",
        "                pred_tokens = pred.split()\n",
        "\n",
        "                # Reference caption as tokens\n",
        "                ref_tokens = [vocab.itos[idx.item()] for idx in true_caption if idx.item() not in [\n",
        "                    vocab.stoi[\"<PAD>\"], vocab.stoi[\"<SOS>\"], vocab.stoi[\"<EOS>\"]\n",
        "                ]]\n",
        "                reference = [ref_tokens]\n",
        "\n",
        "                total_bleu1 += sentence_bleu(reference, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "                total_bleu2 += sentence_bleu(reference, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "                total_bleu3 += sentence_bleu(reference, pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "                total_bleu4 += sentence_bleu(reference, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    bleu1 = total_bleu1 / num_samples\n",
        "    bleu2 = total_bleu2 / num_samples\n",
        "    bleu3 = total_bleu3 / num_samples\n",
        "    bleu4 = total_bleu4 / num_samples\n",
        "\n",
        "    return bleu1, bleu2, bleu3, bleu4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_bleus = evaluate_bleu(model, train_loader, vocab, DEVICE)\n",
        "test_bleus = evaluate_bleu(model, test_loader, vocab, DEVICE)\n",
        "\n",
        "print(\"Train BLEU@1-4:\", train_bleus)\n",
        "print(\"Test  BLEU@1-4:\", test_bleus)\n",
        "\n",
        "\n",
        "\n",
        "bleu_labels = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4']\n",
        "x = np.arange(len(bleu_labels))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(x - 0.15, train_bleus, width=0.3, label='Train', color='skyblue')\n",
        "plt.bar(x + 0.15, test_bleus, width=0.3, label='Test', color='salmon')\n",
        "plt.xticks(x, bleu_labels)\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"BLEU@1-4 Scores\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nMiSAQXrbbt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions for Inference"
      ],
      "metadata": {
        "id": "2njc_wq8cs4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unnormalize(tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Reverses normalization on a tensor image.\n",
        "    Args:\n",
        "        tensor: torch.Tensor of shape [3, H, W]\n",
        "        mean: list of mean values used in normalization (e.g., [0.485, 0.456, 0.406])\n",
        "        std: list of std values used in normalization (e.g., [0.229, 0.224, 0.225])\n",
        "    Returns:\n",
        "        Unnormalized tensor\n",
        "    \"\"\"\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)  # t = t * std + mean\n",
        "    return torch.clamp(tensor, 0, 1)  # clip to valid image range\n",
        "\n",
        "def generate_caption_from_dataset_sample(dataset, model, vocab, device, index=None, max_len=20):\n",
        "    if index is None:\n",
        "        index = torch.randint(0, len(dataset), (1,)).item()\n",
        "\n",
        "    # Get sample from dataset\n",
        "    image_tensor, original_caption = dataset[index]\n",
        "\n",
        "    # Ensure shape is [1, 3, 224, 224]\n",
        "    print(\"Image tensor shape before processing:\", image_tensor.shape)\n",
        "\n",
        "\n",
        "    # Generate caption\n",
        "    model.eval()\n",
        "\n",
        "    generated_caption = model.generate_caption(image_tensor)\n",
        "\n",
        "\n",
        "\n",
        "    original_caption_words = [vocab.itos[idx.item()] for idx in original_caption if idx.item() not in {vocab.stoi[\"<SOS>\"], vocab.stoi[\"<EOS>\"], vocab.stoi[\"<PAD>\"]}]\n",
        "    original_caption_str = ' '.join(original_caption_words)\n",
        "\n",
        "    original_caption_str = ' '.join(original_caption_words)\n",
        "\n",
        "\n",
        "    # Display image\n",
        "    unnorm = unnormalize(image_tensor.squeeze(0).cpu(), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    plt.imshow(F.to_pil_image(unnorm))\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(generated_caption, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    return generated_caption, original_caption_str\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate caption for a random sample from training dataset\n",
        "caption, original = generate_caption_from_dataset_sample(\n",
        "    dataset=train_dataset,  # or test_dataset\n",
        "    model=model,\n",
        "    vocab=vocab,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "print(\"Orginal Caption:\", original)\n",
        "print(\"Generated Caption:\", caption)\n",
        "\n",
        "\n",
        "\n",
        "caption2,_=generate_caption_from_dataset_sample(train_dataset, model, vocab, DEVICE, index=10)\n",
        "caption2,_=generate_caption_from_dataset_sample(train_dataset, model, vocab, DEVICE, index=87)\n"
      ],
      "metadata": {
        "id": "cEnMy3XRbefK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replace the Decoder with LSTMDecoder in model definition to get a model with LSTM as Decoder\n"
      ],
      "metadata": {
        "id": "F_r-gUEUbs4e"
      }
    }
  ]
}